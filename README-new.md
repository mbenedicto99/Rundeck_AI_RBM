# Rundeck AI RBM ‚Äî Detec√ß√£o de Anomalias em Execu√ß√µes

**Rundeck_AI_RBM** √© um pipeline de **ETL ‚Üí features ‚Üí treino RBM ‚Üí detec√ß√£o ‚Üí agrega√ß√£o** para identificar execu√ß√µes an√¥malas (jobs/projetos) usando **Restricted Boltzmann Machine (RBM)**.  
O resultado final √© um JSON can√¥nico **`app/ai_analysis.json`** pronto para UI, APIs e alertas.

Reposit√≥rio: `https://github.com/mbenedicto99/Rundeck_AI_RBM`

---

## ‚ú® Principais benef√≠cios

- **N√£o supervisionado**: dispensa r√≥tulos de anomalia (raros/caros).
- **Sinal cont√≠nuo**: o **erro de reconstru√ß√£o (RE)** da RBM permite thresholds por projeto/job.
- **Aprende depend√™ncias**: captura padr√µes **n√£o lineares** de coocorr√™ncia entre features (tempo, status, dura√ß√£o).
- **Operacionaliz√°vel**: sa√≠da em JSON padronizado, f√°cil de integrar a dashboards e automa√ß√µes.

---

## üó∫Ô∏è Arquitetura (n8n + pipeline)

### Vis√£o completa (orquestra√ß√£o + scripts + artefatos)

```mermaid
flowchart TB
  %% ========= FONTES =========
  S0[(Fonte de dados<br/>Rundeck/Export)]
  S[/data/slice.csv/]

  %% ========= ORQUESTRA√á√ÉO =========
  subgraph N8N[Orquestra√ß√£o ‚Ä¢ n8n]
    direction TB
    Cron[[Cron (agendado)]] --> Exec[Execute Command<br/>source /home/node/venv/bin/activate; python scripts/pipeline.py]
    Webhook[[Webhook /run-pipeline]] -->|input, fileUrl| IF{fileUrl?}
    IF -- "sim" --> DL[HTTP Request (download)]
    DL --> WR[Write Binary File<br/>(/workspace/data/slice.csv)]
    IF -- "n√£o" --> Exec
    WR --> Exec
  end

  S0 -->|Gera/atualiza| S
  S -. opcional (upload via Webhook) .-> WR
  Exec -->|INPUT_CSV=/workspace/data/slice.csv| ETL

  %% ========= CONTAINER / PIPELINE =========
  subgraph CTR[Container n8n-ai ‚Ä¢ venv /home/node/venv]
    direction LR
    ETL[etl.py<br/><sub>normaliza e enriquece</sub>] --> FEAT[features.py<br/><sub>escala p/ [0,1]</sub>] --> TRAIN[train_rbm.py<br/><sub>treina RBM</sub>] --> DET[detect_anomalies.py<br/><sub>erro de reconstru√ß√£o (RE)</sub>] --> BUILD[build_ai_json.py<br/><sub>agrega resultados</sub>]
  end

  %% ========= ARTEFATOS =========
  S2[/data/slice.csv/]
  CL[/data/clean.csv/]
  EXE[/data/execucoes.csv/]
  FE[/data/features.csv/]
  MOD1[[models/scalers.joblib]]
  MOD2[[models/rbm.joblib]]
  SC[/data/score.csv/]
  OUT[[app/ai_analysis.json<br/>(final)]]

  %% ========= ENTRADAS/SA√çDAS POR ETAPA =========
  ETL <-- usa -- S2
  ETL --> CL & EXE

  FEAT <-- usa -- CL
  FEAT --> FE

  TRAIN <-- usa -- FE
  TRAIN --> MOD1 & MOD2

  DET <-- usa -- FE & MOD1 & MOD2
  DET --> SC
  DET -. opcional resumo .-> OUT

  BUILD <-- usa -- EXE & SC
  BUILD --> OUT

  %% ========= CONSUMIDORES =========
  UI[[Frontend / index.html]]
  API[[APIs / Integra√ß√µes / n8n step]]

  OUT --> UI
  OUT --> API

  %% ========= NOTA RBM =========
  NOTE>Benef√≠cios do RBM:<br/>‚Ä¢ N√£o supervisionado (dispensa r√≥tulos)<br/>‚Ä¢ Aprende padr√µes n√£o lineares<br/>‚Ä¢ Score cont√≠nuo (RE) permite thresholds por job/projeto] --- OUT
```

### Mini-mapa (apenas fluxo de arquivos)

```mermaid
flowchart LR
  subgraph DATA[Camada de Dados]
    SLICE[/data/slice.csv/]
    CLEAN[/data/clean.csv/]
    EXEC[/data/execucoes.csv/]
    FEAT[/data/features.csv/]
    SCORE[/data/score.csv/]
    OUT[[app/ai_analysis.json]]
  end

  subgraph MODELS[Modelos]
    SCALER[[models/scalers.joblib]]
    RBM[[models/rbm.joblib]]
  end

  SLICE -->|etl.py| CLEAN
  SLICE -->|etl.py| EXEC

  CLEAN -->|features.py| FEAT

  FEAT -->|train_rbm.py| SCALER
  FEAT -->|train_rbm.py| RBM

  FEAT -->|detect_anomalies.py| SCORE
  SCALER --->|usa| SCORE
  RBM --->|usa| SCORE

  EXEC -->|build_ai_json.py| OUT
  SCORE -->|build_ai_json.py| OUT
```

---

## üìÅ Estrutura do projeto

```
.
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ ai_analysis.json        # sa√≠da final can√¥nica
‚îÇ   ‚îú‚îÄ‚îÄ anomalies.json          # (legado / opcional)
‚îÇ   ‚îî‚îÄ‚îÄ index.html              # exemplo simples de front
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ slice.csv               # entrada bruta
‚îÇ   ‚îú‚îÄ‚îÄ clean.csv               # sa√≠da do ETL
‚îÇ   ‚îú‚îÄ‚îÄ execucoes.csv           # normalizado p/ build final
‚îÇ   ‚îú‚îÄ‚îÄ features.csv            # features em [0,1] + exec_id
‚îÇ   ‚îî‚îÄ‚îÄ score.csv               # exec_id + re (erro de reconstru√ß√£o)
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ scalers.joblib          # scaler + colunas usadas
‚îÇ   ‚îú‚îÄ‚îÄ rbm.joblib              # modelo RBM
‚îÇ   ‚îî‚îÄ‚îÄ feature_meta.json       # metadados (opcional)
‚îú‚îÄ‚îÄ n8n/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile(.alpine)     # base do container (Alpine/compila | Debian/wheels)
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml
‚îÇ   ‚îú‚îÄ‚îÄ start_n8n.sh / stop_n8n.sh
‚îÇ   ‚îî‚îÄ‚îÄ data/                   # estado do n8n (workflows/credenciais)
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ etl.py                  # -> clean.csv / execucoes.csv
‚îÇ   ‚îú‚îÄ‚îÄ features.py             # -> features.csv
‚îÇ   ‚îú‚îÄ‚îÄ train_rbm.py            # -> models/*
‚îÇ   ‚îú‚îÄ‚îÄ detect_anomalies.py     # -> score.csv (+ json leve opcional)
‚îÇ   ‚îú‚îÄ‚îÄ build_ai_json.py        # -> app/ai_analysis.json
‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py             # orquestrador local
‚îÇ   ‚îî‚îÄ‚îÄ simulate_data.py        # dados sint√©ticos para testes
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## ‚öôÔ∏è Como funciona (etapas)

1) **ETL (`scripts/etl.py`)**  
   Entrada: `data/slice.csv` (separador `;` ou `,`).  
   Sa√≠da: `data/clean.csv` e `data/execucoes.csv` com:  
   `projeto, job, exec_id, inicio, status, duracao_s`.

2) **Features (`scripts/features.py`)**  
   Gera `data/features.csv` com `exec_id` +:
   - `duration_sec_mm`, `duration_z_clipped_mm`
   - codifica√ß√£o c√≠clica: `hour_sin_mm`, `hour_cos_mm`, `wday_sin_mm`, `wday_cos_mm`
   - flags: `failed` (status==failed), `high_runtime` (p95 por projeto+job)

3) **Treino (`scripts/train_rbm.py`)**  
   Salva `models/scalers.joblib` (MinMax + colunas) e `models/rbm.joblib` (RBM).

4) **Detec√ß√£o (`scripts/detect_anomalies.py`)**  
   Calcula **RE** (erro de reconstru√ß√£o) com a RBM ‚Üí `data/score.csv` (`exec_id,re`).

5) **Agrega√ß√£o (`scripts/build_ai_json.py`)**  
   Junta `execucoes.csv + score.csv` e produz `app/ai_analysis.json` com:
   - `resumo` (contagens, dura√ß√£o m√©dia, `re_p95_global`)
   - `risco_p95_por_job` (p95 de RE por projeto+job)
   - `hotspots` (top n execu√ß√µes com maior RE)

---

## üöÄ Quickstart (local)

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -U pip
pip install -r requirements.txt

# Coloque seu arquivo de origem em data/slice.csv
python scripts/pipeline.py

# Ver o resumo
jq '.resumo' app/ai_analysis.json
```

Vari√°veis √∫teis (opcionais):
- `INPUT_CSV=data/slice.csv python scripts/etl.py`
- `OUTPUT_CSV=data/clean.csv`
- `EXECUCOES_CSV=data/execucoes.csv`

---

## üïπÔ∏è Orquestra√ß√£o com n8n

### Subir (Docker)

Na pasta `n8n/`:

```bash
# preparar permiss√µes
mkdir -p ./data && sudo chown -R 1000:1000 ./data

# build & up (escolha Dockerfile ou Dockerfile.alpine)
docker compose build --no-cache
docker compose up -d

# criar venv e instalar deps dentro do container
docker exec -it n8n-ai bash -lc '
python3 -m venv /home/node/venv
source /home/node/venv/bin/activate
pip install -U pip
pip install -r /workspace/requirements.txt
'
```

UI: `http://localhost:5678` (credenciais no `docker-compose.yml` / `.env`).

> **Nota Alpine:** se usar `Dockerfile.alpine`, pode ser necess√°rio compilar `scikit-learn` (lento). Scripts auxiliares no diret√≥rio `scripts/` e `n8n/` cobrem esse cen√°rio. **Debian √© recomendado** se quiser apenas wheels.

### Job agendado (Cron)
**Execute Command ‚Üí Command**:
```bash
set -euo pipefail
source /home/node/venv/bin/activate
cd /workspace
INPUT_CSV=/workspace/data/slice.csv python scripts/pipeline.py
```
**Options ‚Üí Timeout**: ajuste conforme volume (ex.: 600000 ms).

### Job on-demand (Webhook)
- **Webhook** (`/run-pipeline`) com `GET/POST`
- (Opcional) **HTTP Request** baixa CSV de `fileUrl` ‚Üí **Write Binary File** em `/workspace/data/slice.csv`
- **Execute Command**:
  - **Command**:
    ```bash
    set -euo pipefail
    source /home/node/venv/bin/activate
    cd /workspace
    python scripts/pipeline.py
    ```
  - **Options ‚Üí Environment Variables**:
    - Name: `INPUT_CSV`
    - Value (Expression): `={{$json.input || "/workspace/data/slice.csv"}}`
- **Read Binary File** `/workspace/app/ai_analysis.json` ‚Üí **Respond to Webhook**

Testes:
```bash
# GET simples
curl "http://<IP>:5678/webhook/run-pipeline"

# POST (troca o input)
curl -X POST "http://<IP>:5678/webhook/run-pipeline"   -H "Content-Type: application/json"   -d '{"input":"/workspace/data/slice.csv"}'
```

---

## üì¶ Formatos de dados

### `data/execucoes.csv` (ETL)
| coluna     | tipo     | descri√ß√£o                                  |
|------------|----------|---------------------------------------------|
| projeto    | str      | projeto/folder                              |
| job        | str      | nome do job                                 |
| exec_id    | str      | id √∫nico da execu√ß√£o                        |
| inicio     | datetime | in√≠cio                                      |
| status     | str      | `success`/`failed`/‚Ä¶                        |
| duracao_s  | float    | segundos                                    |

### `data/features.csv`
| coluna                  | tipo  | obs                                           |
|-------------------------|-------|-----------------------------------------------|
| exec_id                 | str   | chave                                         |
| duration_sec_mm         | float | min‚Äìmax global                                |
| duration_z_clipped_mm   | float | z-score clipado [-3,3] ‚Üí [0,1]                |
| hour_sin_mm/hour_cos_mm | float | codifica√ß√£o c√≠clica 24h                       |
| wday_sin_mm/wday_cos_mm | float | codifica√ß√£o c√≠clica 7 dias                    |
| failed                  | int   | 1 se status==failed                           |
| high_runtime            | int   | 1 se dura√ß√£o > p95 por projeto+job            |

### `data/score.csv`
| coluna  | tipo  | descri√ß√£o                       |
|---------|-------|----------------------------------|
| exec_id | str   | chave                            |
| re      | float | erro de reconstru√ß√£o da RBM      |

### `app/ai_analysis.json`
```json
{
  "resumo": {
    "total_execucoes": 1234,
    "por_status": {"success": 1200, "failed": 34},
    "duracao_media_s": 42.1,
    "re_p95_global": 0.031
  },
  "risco_p95_por_job": [
    {"projeto":"A","job":"X","re_p95":0.06}
  ],
  "hotspots": [
    {"projeto":"A","job":"X","exec_id":"...","inicio":"2025-09-18T10:10:10","status":"success","duracao_s":55.0,"re":0.08}
  ],
  "top_amostras": [ ... ]
}
```

---

## üß† RBM em 30 segundos

- **BernoulliRBM (sklearn)**: modelo energ√©tico n√£o supervisionado.
- **Entrada**: features em **[0,1]** (j√° normalizadas em `features.py`).
- **Sinal**: **RE = mean((V ‚àí V_recon)¬≤)**, onde `V_recon = rbm.gibbs(V)`.
- **Interpreta√ß√£o**: RE alto ‚Üí execu√ß√£o pouco ‚Äúexplic√°vel‚Äù pelo padr√£o aprendido ‚Üí candidata a anomalia.  
  Use `re_p95_global` e `re_p95_por_job` para limiares e prioriza√ß√£o.

---

## üß™ Dados sint√©ticos (teste)

```bash
python scripts/simulate_data.py > data/slice.csv
python scripts/pipeline.py
jq '.resumo' app/ai_analysis.json
```

---

## üõ†Ô∏è Troubleshooting

- **Coluna obrigat√≥ria ausente em `execucoes.csv`**  
  Verifique o ETL: precisa gerar `projeto, job, exec_id, inicio, status, duracao_s`.

- **MinMaxScaler: Found array with 0 sample(s)**  
  `features.csv` vazio ‚Üí confira `clean.csv` e o parsing do `slice.csv`.

- **No n8n: `ModuleNotFoundError: pandas`**  
  Dentro do container:
  ```bash
  docker exec -it n8n-ai bash -lc 'source /home/node/venv/bin/activate && pip install -r /workspace/requirements.txt'
  ```

- **Alpine pedindo gcc/meson**  
  Use `Dockerfile.alpine` + script de build (compila `scikit-learn` com `SKLEARN_NO_OPENMP=1`).  
  Se n√£o quiser compilar, use a imagem **Debian**.

- **Permiss√£o `/home/node/.n8n/config`**  
  No host: `sudo chown -R 1000:1000 n8n/data && docker compose up -d`.

---

## üîê Boas pr√°ticas

- Paths can√¥nicos: `data/`, `models/`, `app/`.
- `exec_id` est√°vel entre etapas.
- Se expor Webhook, **autentique** (Basic/Auth token) e restrinja por rede.
- Fa√ßa **backup** de `n8n/data` (workflows/credenciais).

---

## ü§ù Contribuindo

1. Crie uma branch a partir de `main`.  
2. Commits pequenos e descritivos.  
3. PR com descri√ß√£o do problema/solu√ß√£o e passos de teste.

---

## üì§ Publicar no GitHub

```bash
cd ~/Documents/CanopusAI/RUNDECK_AI
git init
git remote add origin git@github.com:mbenedicto99/Rundeck_AI_RBM.git
git checkout -b main
git add .
git commit -m "docs: README completo + pipeline RBM"
git push -u origin main
```

> HTTPS: `git remote add origin https://github.com/mbenedicto99/Rundeck_AI_RBM.git`

---

## üìÑ Licen√ßa

Escolha e adicione `LICENSE` (ex.: MIT).
